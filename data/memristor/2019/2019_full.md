# **Memristive crossbar arrays for brain-inspired computing**

**Qiangfei Xia  \* and J. Joshua Yang   \***

**With their working mechanisms based on ion migration, the switching dynamics and electrical behaviour of memristive devices resemble those of synapses and neurons, making these devices promising candidates for brain-inspired computing. Built into large-scale crossbar arrays to form neural networks, they perform efficient in-memory computing with massive parallelism by directly using physical laws. The dynamical interactions between artificial synapses and neurons equip the networks with both supervised and unsupervised learning capabilities. Moreover, their ability to interface with analogue signals from sensors without analogue/digital conversions reduces the processing time and energy overhead. Although numerous simulations have indicated the potential of these networks for brain-inspired computing, experimental implementation of large-scale memristive arrays is still in its infancy. This Review looks at the progress, challenges and possible solutions for efficient brain-inspired computation with memristive implementations, both as accelerators for deep learning and as building blocks for spiking neural networks.**

The computing capability of digital computers, based on complementary metal oxide semiconductor (CMOS) transistors, has increased greatly in the past few decades, mainly through continual shrinking of the transistor dimension as predicted by Gordon Moor[e1](#page-12-0) . However, the way in which hardware components are organized into a functional computer, namely the von Neumann architecture, has barely changed since its inception in 194[52](#page-12-1) . This is partially due to the great advantage of such architecture in modularity of engineering design, which enables thousands of engineers to work independently in building an extremely complicated system without the need to understand all components. Nevertheless, with the advent of the Internet of Things, an exponential growth in the amount of data has imposed critical requirements on the energy efficiency and processing speed for data-centric tasks. Therefore, the drawbacks of traditional digital computers are an increasing problem.

On the device level, leakage currents become an issue as the channel length and the thickness of gate dielectric of a transistor approach the scaling limit. On the architecture level, the constant data shuttling between the information processing and memory units in the von Neumann architecture significantly limits the speed and energy efficiency (the 'von Neumann bottleneck'). In addition, the performance mismatch between the two units leads to considerable latency (referred to as 'memory wall') in the von Neumann architecture. Modified architectures have been adopted to enhance the computing capability and efficiency. For example, graphics processing units (GPUs) with multiple cores and high-throughput interconnection are among fairly successful attempts to increase the parallelism in computin[g3](#page-12-2) . When GPUs are used for neural networks, the synaptic weights are still stored in separated units, such as static random access memory (SRAM), that need to be visited frequently for data fetching and constantly powered up to store the information. The tensor processing unit, a type of applicationspecific integrated circuit, demonstrates a further improved power efficiency by using low-precision computation at high volume, but the latency issue still remains[4](#page-12-3) .

The ability to perform computing at the site where data is stored ('in-memory computing') is resurging as an alternative to current computing schemes. First proposed in the 1960[s5](#page-12-4) , the concept of inmemory computing was initially demonstrated in the digital domain. However, it was not paid enough attention, probably because of the satisfying improvement of computing capability fuelled by the rapid development of transistors in the past few decades. Various emerging electronic devices, taking advantage of physical phenomena such as spin, phase transition or ionic transport[6–](#page-12-5)[9](#page-12-6) , are becoming more mature both in physical understanding and technological developments. New computing systems using the in-memory computing concept have built upon these beyond-CMOS devices and nanotechnology, offering an attractive solution to the energy consumption and speed issues. Among these, the memristor stands out as a promising candidate.

The memristor, also known as a resistance switch, is an electronic device whose internal states are dependent on the history of the current and/or voltage it has experienced[10](#page-12-7)[,11](#page-12-8). The programming of a conductance requires only a small amount of energy, as the device can be extremely small (<2 nm)[12](#page-12-9), and the switching speed can be very fast (<1 ns[\)13](#page-12-10). Organized into large arrays, or stacked three-dimensionally (3D), the intermediate computing results (such as weights in a neural network) can be stored locally as conductance in each single, non-volatile memristor during computing. At each cross point, the current is the product of input voltage and memristor conductance, following Ohm's law for multiplication. At each column, the total current is a summation of the current at each cross point according to Kirchhoff 's current law. Together, they implement vector matrix multiplication, which is the core computing task in deep learning algorithms but very resource-expensive for traditional digital computing systems. Furthermore, the current sensing is finished at once, regardless of the matrix size, offering a huge parallelism that leads to superior computing throughput. The physical computing enables direct interfacing with analogue signals from sensors. This could potentially lead to analogue computing without using analogue-to-digital or digital-to-analogue (ADC/ DAC) conversions, which typically consume most of the energy in a mixed-signal neural network. In addition, the electrical conductance change of most memristors is a result of ion motion, similar

Department of Electrical and Computer Engineering, University of Massachusetts, Amherst, MA, USA. \*e-mail: [qxia@umass.edu;](mailto:qxia@umass.edu) [jjyang@umass.edu](mailto:jjyang@umass.edu)

![](_page_1_Figure_2.jpeg)

<span id="page-1-0"></span>**Fig. 1 | From materials science to artificial intelligence.** Physical properties of materials, such as conductance, can be used to represent synaptic weights in neural networks. The conductance of a memristor can be modulated through different mechanisms. Shown in the bottom panel, with schematics and exemplary micrographs, are three typical conductance tuning mechanisms, by changing: the width of a conduction channe[l87](#page-13-0), the gap between the channel and an electrode[38,](#page-12-21) and the composition of the conductive channel[36](#page-12-22). Consequently, a memristor crossbar array (an example is shown in the central panel[88\)](#page-13-1) will carry different and tunable synaptic weights in each cell, forming a computational framework with a broad spectrum of artificial intelligence applications (top panel). Reproduced from ref. [87](#page-13-0), SNL (bottom left). Adapted from ref. [88,](#page-13-1) IOP (middle); ref. [36,](#page-12-22) SNL (bottom right); and ref. [38,](#page-12-21) American Chemical Society (bottom middle).

to those observed in biological synapses and neurons. These unique features make memristors a natural choice as building blocks for neural networks with a broad spectrum of applications such as computer vision, speech recognition, autonomous vehicles, robotics, medicine and finance, to name a few (Fig. [1](#page-1-0), top panel).

Extensive numerical simulations have showed that memristive neural networks will bring in orders of magnitude higher speed– energy efficiency when compared with traditional CMOS hardware platform[s14](#page-12-11). However, experimental demonstrations to this end have been limited to very small arrays that can only solve relatively simple problems, mainly because of non-idealities in device properties (such as variability) and challenges in large array integration. Table [1](#page-2-0) summarizes a few successful demonstrations using one-transistor-one-resistance switch (1T1R) arrays for analogue computing and multilayer neural networks, and passive arrays for computation purposes. A 128 × 8 1T1R array of HfAl*y*O*x*/TaO*x* devices was the first experimental implementation for face recognition[15](#page-12-12), and a 128 × 64 Ta/HfO2 1T1R array was built and used for efficient analogue signal and image processing and machine learnin[g16](#page-12-13)[–19](#page-12-14). A 10 × 6 portion in a 12 × 12 array of Al2O3/TiO2 memristors was used to recognize 3 × 3 pixel black/white images[20.](#page-12-15) A 16 × 32 subarray from a 32 × 32 crossbar array of WO*x*-based memristors was also used for image processing[21.](#page-12-16) In addition to these resistive neural networks based on memristors, capacitive neural networks based on memcapacitive devices have just started to be explored experimentally as an alternative for potentially further improved energy efficienc[y22.](#page-12-17) For memristive neural networks to be used for more complicated computing tasks, besides many other challenges in circuit and system designs, the array size must be much larger, and peripheral circuits must be integrated with the computing kernels for optimal overall efficiency of energy and throughput.

This Review will summarize recent efforts in building memristive crossbar arrays, as well as experimental demonstration of computing functionalities in such networks. We will focus on two common types of neural networks, namely deep neural networks (DNNs) and spiking neural networks (SNNs), and discuss issues with network inference and training. Inference is the computation process, while training is the process in which synaptic weights in a neural network are tuned according to certain algorithms. More fundamental concepts in brain-inspired computing are briefly explained in Box 1. More importantly, we intend to identify challenges and propose possible solutions in the transition from discrete device-level research to large-scale memristive crossbar arrays for brain-inspired computing. Readers who are interested in single device research as well as array simulations are encouraged to explore already abundant review articles on those topics[23–](#page-12-18)[29](#page-12-19).

### **Deep neural networks**

Thanks to the progress in CMOS scaling, algorithm development and architecture design, DNNs have been one of the most powerful tools to achieve milestones in artificial intelligenc[e30.](#page-12-20) DNNs

|                            | Active arrays (1T1R)                                                                                                             |                                                                 | Passive arrays                                                                  |                                                                                    |                                      |  |
|----------------------------|----------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------|---------------------------------------------------------------------------------|------------------------------------------------------------------------------------|--------------------------------------|--|
| Memristor geometry         | Ta/HfO2/Pt or Pd                                                                                                                 | TiN/HfAlyOx/TaOx/TiN                                            | Pt/Al2O3/TiO2−x/Ti/Pt                                                           | W/WOx/Pd/Au                                                                        | Pt/TiO2/Pt                           |  |
| Memristor size             | 4 × 4 μm2                                                                                                                        | NA                                                              | 200 × 200 nm2                                                                   | 500 × 500 nm2                                                                      | NA                                   |  |
| Transistor node            | 2 μm                                                                                                                             | 1.2 μm                                                          | No transistors                                                                  |                                                                                    |                                      |  |
| Max. array size            | 128 × 64                                                                                                                         | 128 × 8                                                         | 20 × 20                                                                         | 32 × 32                                                                            | 4 (wired) + 4<br>(software)          |  |
| Array image                | 500 µm                                                                                                                           | 128 × 8<br>1,024-cell<br>1T1R array<br>200 µm                   | 2 µm                                                                            | 30.0 µm                                                                            | NA                                   |  |
| Multilevel mechanism       | Composition modulation<br>of a Ta-rich conduction<br>channel                                                                     | Modulation of the local<br>concentration of oxygen<br>vacancies | Reversible modulation<br>of the concentration<br>profile of oxygen<br>vacancies | Modulation of the<br>area ratio between<br>a Schottky and a<br>tunnelling junction | NA                                   |  |
| Device retention           | >>10 years at 85 o<br>C                                                                                                          | NA                                                              | >50,000 seconds                                                                 | Tunable                                                                            | 2.5 hours                            |  |
| Conductance range          | 100–900 μS                                                                                                                       | 10–60 μS                                                        | 12–142 μS                                                                       | 0–5 μS                                                                             | 145–175 μS                           |  |
| I–V linearity              | Linear                                                                                                                           | Nonlinear                                                       | Nonlinear                                                                       | Nonlinear                                                                          | NA                                   |  |
| Synapse precision          | 0.75% error                                                                                                                      | 5% or 35% error                                                 | 7.4% error                                                                      | 5.6% errora                                                                        | NA                                   |  |
| Weight update scheme       | Set/reset/gate amplitudes                                                                                                        | Identical pulses                                                | Identical pulses                                                                | Identical pulses                                                                   | Set/reset<br>amplitudes              |  |
| Symmetry in weight update  | Symmetric                                                                                                                        | Asymmetric                                                      | Asymmetric                                                                      | Asymmetric                                                                         | Asymmetric                           |  |
| Peripheral circuitry       | Off-chip MCU with DAC/<br>ADC, TIA                                                                                               | SPA + switching matrix                                          | SPA + switching<br>matrix + PCB<br>hardware neuron                              | Off-chip MCU with<br>DAC/ADC, TIA                                                  | Off-chip MCU<br>with DAC/ADC,<br>TIA |  |
| Application demonstrations | Image compression/<br>filtering; MNIST<br>classification; time-series<br>regression; gait recognition;<br>reinforcement learning | Face classification                                             | 3 × 3 or 4 × 4 bitmap<br>classification                                         | Sparse coding;<br>reservoir computing                                              | Unsupervised<br>learning             |  |
| References                 | 16–19,36                                                                                                                         | 15                                                              | 20,89                                                                           | 21,90                                                                              | 91                                   |  |

<span id="page-2-0"></span>

| Table 1   Figures of merit for experimentally demonstrated analogue memristor crossbar arrays |  |  |  |  |  |
|-----------------------------------------------------------------------------------------------|--|--|--|--|--|
|-----------------------------------------------------------------------------------------------|--|--|--|--|--|

a Calculated from the reported nine levels of conductance and 1 μS standard deviation of the device. MCU, microcontroller unit; PCB, printed circuit board; SPA, semiconductor parameter analyser; TIA, transimpedance amplifier. Images, left to right, reproduced from ref. [16](#page-12-13), SNL; ref. [15](#page-12-12), SNL; ref. [20,](#page-12-15) SNL; adapted from ref. [21](#page-12-16), SNL.

can also be implemented with memristive crossbars, using discrete conductance levels (or capacitance levels for a capacitive neural network) to represent synaptic weights in a massively parallel fashion. The neurons that connect each layer, mostly implemented with amplifiers based on CMOS circuitry thus far, could potentially be built with different types of memristor[s31–](#page-12-23)[33.](#page-12-24)

**Key device requirements.** In principle, neural networks with memristive synapses require the memristors to have a wide resistance range to accommodate a large number of resistance states. Low fluctuation and drift in each resistance state and high absolute resistance values are required for the inference, while a high endurance is necessary for repeated programming/training. The choice of switching materials is critical to achieve high endurance. A metal-oxide system with two and only two stable solid phases (at the temperature at which switching occurs), that is, a conductive and an insulating phase, may be beneficial for stable switching with high endurance. HfO*x* and TaO*x* are two typical examples of such a metal-oxide system, while TiO*x* is not[34.](#page-12-25)

Multiple resistance states are achieved through different mechanisms for different types of memristive devices. The lower panels of Fig. [1](#page-1-0) summarize the typical approaches to tune the resistance of each individual cell in a crossbar array (middle panel). First, the resistance can be tuned through the modulation of the size or volume, and/or the composition, of a localized conduction channel(s)[35–](#page-12-26)[37.](#page-12-27) Tuning the diameter of a channel for a device of the conduction bridge type is usually done by applying different levels of compliance. However, once a metallic bridge is formed, reducing its size is usually an abrupt process. Modulating the distance of a tunnelling gap is an efficient approach to achieve analogue behaviour[38,](#page-12-21) which usually leads to a wide resistance range. But the current–voltage relationship is nonlinear, as the tunnelling current depends exponentially on the gap size. It seems that composition modulation in the conduction channel might be the most practical approach to obtain analogue resistance levels. In a three-terminal ionic transistor, the conductance of the channel can be modulated by tuning the concentration gradient of extrinsic doping electrically controlled by the gate. For instance, proton-doped polymer materials as the channel of an ionic transistor have resulted in up to 500 levels of conductance states[39](#page-12-28). Electric pulses, either with different voltage amplitudes or pulse widths/ numbers, are usually used to programme the device into different conductance states ('weight updating' in neural network terminology). To achieve the target conductance value with high accuracy, it may take more than one programming pulse, which inevitably increases the energy consumption. On the other hand, lower-precision computing with a proper algorithm can greatly reduce the energy overhead and has become popular as it is often sufficient for machine learning applications.

Device stability is critical to the computing accuracy, as drift of conductance states with time and/or environmental changes will result in undesired synaptic weight change. This has been one of the chief challenges for many memristive systems, especially for phase-change materials based on chalcogenides[40](#page-12-29). Device engineering has been adopted to address such issues. For example, the conductance states have been demonstrated to be fairly stable for a Ta/ HfO2 hybrid device with a Ta(O) conduction channel in the HfO2 matri[x36](#page-12-22). This is because of the high activation energy of Ta species in the conduction channel, suggesting that mobile species with high activation energy are preferred to minimize the drift of conductance states and disturbance during readout.

Variation in programming devices to a target conductance will blur the distinction in resistance levels and is thus a major limiting factor to the bit precision in analogue computing. Because the resistance switching behaviour essentially relies on the motion of a small number of ions, this variation is intrinsic to memristive devices[41.](#page-12-30) Efforts in device engineering, such as using dislocation to constrain the size of a conduction channel, have been proposed and demonstrated[37.](#page-12-27) By connecting two memristors serially with a minimum-sized transistor and using the ratio of the resistances of the memristors to encode information, variation can be greatly reduced as well[42](#page-12-31). In addition to these material and circuit approaches, an operation protocol to reduce the device variation using closed-loop peripheral circuits with 'write-verify' functionality has also been demonstrate[d43](#page-12-32). Many techniques adopted in other emerging devices could be borrowed to reduce the memristive device variabilit[y44.](#page-12-33) The intrinsic stochasticity of memristive switching behaviour, on the other hand, can be a useful feature in emulating biological systems, in which stochasticity is believed to have a key role in the error-tolerance and robustness of computing with low-precision and 'noisy' devices. In addition, a minor degree of noise in device states is beneficial for applications equivalent to an optimization problem as it helps the computing system to escape from local minima and reach the global minimum. This has been experimentally confirmed recentl[y17](#page-12-34), implying that memristors with their intrinsic noise might be more suitable for neural computing rather than memory or storage applications.

A linear current–voltage (*I–V*) relationship is necessary to make direct use of Ohm's law to compute, where electric pulses with different amplitudes can be conveniently used for multiplication computing. Unfortunately, this results in an undesirably high current as a linear *I–V* usually occurs in a high conductance range of memristors. How to use the lower conductance range, in which *I–V* is usually nonlinear and the resistance levels tend to have a larger variation, is a critical topic of research. A common practice is to interrogate the conductance states of memristors with pulses of a fixed amplitude but different widths (or equivalently different numbers of pulses) and measure the output charge as the multiplication result. With a fixed amplitude, a linear relationship between the measured charge (*Q*) and pulse width (*t*) is established through *Q* = *VGt* where *V* and *G* are voltage amplitude and conductance, respectively. As such, a lower range of conductance could be used for computing, leading to reduced power consumption in the array. The peripheral circuits for pulse width modulation could also be simple[r45,](#page-12-35)[46](#page-12-36).

Last but not least, linear and symmetric weight updating is required for efficient network training. This means that the conductance of the device should increase (potentiate) at one polarity and decrease (depress) at the other, and the amount of potentiation and depression should be the same for each electrical pulse. However, most memristive devices exhibit nonlinear dynamics in the switching behaviour with asymmetric potentiation/ depression. To improve the symmetry, several techniques, both at device and circuit levels, have been proposed and demonstrated. For example, two devices with opposite weights were connected to compensate for the asymmetry[47](#page-12-37). Other proposed approaches include optimizing programming pulses for the device[s48](#page-12-38) or adopting weight-updating rules that are more tolerant to the nonlinear programming[49](#page-12-39).

Some of these key figures of merit for experimentally demonstrated arrays are reported in Table [1](#page-2-0). It is difficult to achieve all required properties from one memristive device. A Phase-change memory (PCM) relies on partial amorphization/crystallization to achieve multiple conductance states, but the conductance tuning is hardly linear. One solution is to use multiple PCMs and transistors wired together to represent one synaps[e50](#page-12-40), which leads to a higher programming energy and a larger footprint. Filamentary memristors based on the formation and rupture of a conduction channel within a dielectric layer have a high dynamic range, but the channel formation and rupture tend to be abrupt instead of gradual. In contrast, for a non-filamentary memristor based on the continuous tuning of the Schottky barrier width, the retention property needs improvement for inference systems, whereas this is less of an issue for an online training syste[m51](#page-12-41),[52.](#page-12-42) Compared with mature techniques such as FLASH memory-based neural networks[53,](#page-12-43) which suffer from issues intrinsic to FLASH (such as high voltage, slow switching and limited endurance), more efforts in research and development are needed for memristors. Device engineering, together with the proper design of algorithms and peripheral circuits, is expected to fully unleash the potential of memristive devices as electronic synapses and neurons in neural networks.

**Active arrays.** The most practical approach to achieve a large memristor crossbar array at this moment is to integrate memristors with MOS transistors, even considering the cost of a larger circuit footprint. With a transistor connected to a memristor in series (and hence a 1T1R cell: Table [2](#page-4-0), left), the current flowing through unselected cells ('sneak path current') can be efficiently suppressed to enable accurate reading and programming of the memristors. The third terminal (gate) in the transistors further offers controllability in updating the synaptic weight in a linear and symmetric manner during the training of memristor neural network[s19](#page-12-14). Individual control of each gate voltage enables semi-parallel programming (row by row or column by column) of the entire array, increasing training efficiency. The maturity of the CMOS industry makes the manufacturing of a large 1T1R array with high yield feasible. Examples of properties and images of experimentally demonstrated active arrays are also shown in Table [1](#page-2-0).

During computing operations (inference), all transistors in the 1T1R array are in the ON state to minimize the effect of channel resistance on the crossbar array operation; while during memristor conductance programming (training), transistors are partially ON to allow for precise weight updating. Transistors with minimum channel resistance in the ON state should be chosen when possible. This is likely to lower the packing density of the array, but it is less of an issue for computing than for memory or storage applications. The energy overhead from the transistors should also be taken into account. Depletion-mode transistors that are in the ON state at zero gate-source voltage are favoured for the inference, but an increased channel leakage for such transistors, and thus power consumption during the conductance programming (training) process, could be a concern. Therefore, the trade-off between the inference and training needs to be considered when choosing a proper transistor.

**Passive arrays.** The passive array, which has no transistors in the memristor crossbar, is attractive for its potential in low power consumption and high packing density, but the sneak path current and half-select issues (see Fig. [2](#page-4-0)) usually prevent one from appropriately programming or reading memristors in a large passive array. One approach to mitigate these issues is to engineer the *I–V*

### <span id="page-4-0"></span>**Table 2 | Comparison between 1T1R and 1S1R architecture**

![](_page_4_Figure_3.jpeg)

In a 1T1R array, the drain of a transistor is connected to a memristor. The series transistor precisely controls the current flowing through the memristor for multilevel conductance programming. The transistor also mitigates the sneak path current and half-select issues during array programming and reading. The half-select issue refers to the case where the half-selected or partially selected memristors (those memristors sharing the same row or column with the target memristor) are partially programmed as a result of non-zero voltage drops on them. Transistors in a 1T1R array, however, may reduce the packing density and limit 3D stackability of the circuits. In a 1S1R array, a two-terminal selector with a high nonlinearity in its *I–V* characteristic is connected to the memristor, usually into one stack with the same footprint (4*F*<sup>2</sup> , where *F* is the half pitch), leading to much higher packing density and 3D stackability. Both 1T1R and 1S1R arrays can be programmed in a semi-parallel fashion. However, the reading of a 1S1R array in vector matrix multiplication is limited to pulse width or number modulation in order to use the different format of Ohm's law because of the nonlinearity in *I–V*. Cross-sectional micrographs reproduced from ref. [16](#page-12-13), SNL (left); ref. [92](#page-13-5), Wiley (right). Left array programming sketch adapted from ref. [17,](#page-12-34) SNL.

nonlinearity of the memristor itself[15](#page-12-12)[,16,](#page-12-13)[54](#page-12-44), which is attractive since it does not need an extra access device in each junction in a large array. On the other hand, the *I–V* nonlinearity limits the choice of inference to pulse width/number modulation only. A potential solution could be a rectifying device with an analogue linear forward *I–V*, or a nonlinear device with linear *I–V* within a certain voltage range.

Another common solution to the sneak path problem is to connect in series a two-terminal selector device in each cell (and hence a 1S1R architecture: Table [2](#page-4-0), right). In this architecture, the memristor and selector can be stacked on top of each other, leading to a smaller cell footprint than the 1T1R scheme. When a bipolar memristor is connected in series with a selector, the combined device has nonlinear behaviour with a much lower current at half of the read voltage (Fig. [2a](#page-5-0)). Figure [2b](#page-5-0) illustrates a 'half-bias' read scheme, in which the top electrode of the selected cell is biased at full voltage (*V*r) and the bottom electrode is grounded, while all other wires are biased at half voltage. The sneak path current will limit the size of an array. Usually a higher nonlinearity allows a larger operational passive crossbar array under otherwise identical conditions. For example, with a nonlinearity of 105 in the *I–V* relation, an operational array with over 1010 cells can be realized assuming a readout margin of 10% (Fig. [2c\)](#page-5-0).

An ideal selector needs to be electroforming-free, scalable and stackable with high nonlinearity, high endurance, high speed for both ON and OFF switching, sufficient current density, low energy and low variability. Table [3](#page-6-0) summarizes reported two-terminal, thin-film-based selectors with known geometry, including Schottky diodes, tunnelling junctions, ovonic threshold switches, metal–insulator transitions, diffusive memristor and self-rectifying devices, with the best reported figures of merit highlighted in bold fonts. It is clear from Table [3](#page-6-0) that there is no ideal selector yet that possesses all required properties. Tunnelling-based (or electronic) selectors have advantages such as fast speed, high repeatability, forming-free switching, infinite endurance and very small temperature dependency. However, they typically have limited

**a**

![](_page_5_Figure_2.jpeg)

<span id="page-5-0"></span>**Fig. 2 | Impact of device nonlinearity on the capacity of a passive array. a**, Connecting a bipolar memristor (*I*–*V* shown in the left panel) with a bidirectional selector (*I*–*V* in the middle) yields a characteristic *I–V* curve (right) with a much lower current at half read voltage (*V*<sup>r</sup> /2) than that of the original memristor, resulting from the nonlinearity of the selector device. **b**, Schematic illustration of a crossbar array during read operation. The top electrode of the selected cell (in blue) is biased at full voltage (*V*r) and the bottom electrode is grounded, while all other wires are biased at half voltage. Consequently, devices that share one electrode with the selected device experience half bias ('half-selected devices'), leading to sneak path current in the array. **c**, With higher nonlinearity, defined as the ratio of the current at *V*r to the current at *V*<sup>r</sup> /2, the sneak path current is effectively suppressed, and hence a much larger array can be operational under the same readout margin (window between current read at high- and low-resistance states).

nonlinearity and current density. One major issue of the selectors based on metal–insulator transitions is insufficient resistance in the OFF state owing to the small bandgaps normally associated with those materials. Selectors based on ion motion have shown over 1 × 1010 nonlinearity, but normally with a larger variance than electronic selectors. The performance requirements for a selector are even more demanding than those for a memristor, partially because selectors are switched not only during memristor programming but also during the more frequent reading operations. Electroforming is a major source of device-to-device variance for both ionic-based selectors and memristors and needs to be avoided in large crossbar arrays. In addition, the specific properties and operation parameters (such as threshold voltage and operation current) of the selector need to match those of the memristors to achieve the required *I–V* nonlinearity from the combined device, which requires careful co-design between the selector and memristor. It should be pointed out that most selector work listed in Table [3](#page-6-0) is at the individual device level. Experimentally integrated 1S1R arrays are not widely reported, especially for neuromorphic computing applications. Recent array-level simulation shows that the accuracy of the weighted sum is dependent on selector characteristics such as nonlinearity[55](#page-12-45).

Because of these challenges, the size of passive arrays remains small (but with high integration density). Consequently, the application of passive arrays before an ideal selector is developed is likely to be limited to convolutional neural networks, such as filters, where no large array is necessary. In this case, small-weight kernels are used over and over again in the convolutional neural networks, for which digital accelerators remain competitive.

## **Spiking neural networks**

SNNs encode information in the timing of spikes and compute only when events occur. SNNs emulate the brain more faithfully and are envisaged to be more energy efficient, especially for processing temporal data such as speech and videos[56.](#page-12-46) Similar to DNNs, SNNs implemented with memristive devices do not need a separated unit to store synaptic weights, bypassing the von Neumann bottleneck with improved efficiency. More importantly, SNNs can conveniently implement hardware-coded physiological spatiotemporal learning such as spike-timing-dependent plasticity (STDP) learning with Hebbian rules[57](#page-12-47). SNNs are also believed to be noise-resilien[t58,](#page-12-48) further reducing the overhead in peripheral circuits that are otherwise needed for better computing precision in other types of neuromorphic hardware. Despite their great potential, the computational capability of SNNs has not been demonstrated as much as DNNs, mainly because of the lack of efficient algorithms and dynamic devices. It is hence believed that DNNs are a near-term application for memristor crossbar arrays, whereas SNNs, without an efficient algorithm, are a long-term goal for researchers in this field.

**Prior art.** CMOS circuits running in current mode, voltage mode and subthreshold mode have been used to implement SNN[s59.](#page-12-49) Examples include Neurogrid[60,](#page-12-50) SpiNNaker[61,](#page-12-51) TrueNorth[62](#page-12-52) and Loihi[63.](#page-12-53) The programmable neuromorphic chip Neurogrid provides biological real-time simulations of a million neurons and their synaptic connections. With an assembly of Neurocore chips, the system features ten 256 × 256 matrices of spiking neurons. However, it takes tens of transistors and capacitors to simulate a neuron in these analogue CMOS circuits. IBM's TrueNorth SNN

<span id="page-6-0"></span>

| Mechanism                  | Selector geometry                                         | Non-linearity | Operation       | Max.                               | Speed (ns)b |       | Endurance     | I–V      |
|----------------------------|-----------------------------------------------------------|---------------|-----------------|------------------------------------|-------------|-------|---------------|----------|
|                            |                                                           |               | voltage<br>(V)a | current<br>density<br>(A cm−2<br>) | ton         | tof   |               | symmetry |
| Bidirectional selectors    |                                                           |               |                 |                                    |             |       |               |          |
| Si PN junction             | Punch through NPN diodes93                                | 250–4,700c    | –3 to 3.8       | >106                               | <10         | NR    | NR            | No       |
| Schottky barrier           | Ni/TiO2/Ni94                                              | 1×104         | ±2.4            | 1×105                              | NR          | NR    | 103<br>(d.c.) | Yes      |
|                            | Pt/TiO2/TiN95                                             | ~100          | ±2              | 103<br>to<br>1.5×104               | NR          | NR    | NR            | No       |
| Tunnelling                 | TiN/Ta2O5/TiN96                                           | 360           | ±2              | 1×105                              | <10         | <10   | 107           | Yes      |
|                            | W/Ta2O5/TaOx/ TiO2/TiN97                                  | 103 c         | ±2              | >107                               | <20         | <10   | 105           | Yes      |
|                            | Pt/TaOx/TiO2/TaOx/Pt98                                    | 1×104         | ±1.5            | 3.2×107                            | 100         | NR    | 1010          | No       |
|                            | Pt/TaN1+x/Ta2O5/TaN1+<br>x/Pt99                           | 1.1×104       | ±2              | ~104                               | NR          | NR    | 108           | Yes      |
|                            | TaN/SiNx/TaN100                                           | 140           | ±2              | 1×105                              | <10         | NR    | NR            | Yes      |
| Ionic transport            | TE(Cu doped) /Cu8GeSe6/<br>BE(Cu doped)101                | 1×107         | ±1              | 1.5×107                            | 2.5×104     | 15    | 1010          | Yes      |
|                            | TiN/GexSe1–x/TiN102                                       | 3×103         | ±1.5            | 2×107                              | <10         | NR    | 106           | Yes      |
|                            | Boron/Carbon (BC) based<br>Ovonic threshold switch103,104 | 1×104         | ±3.2            | 1×107                              | <10         | NR    | 108           | Yes      |
|                            | Doped-chalcogenide105                                     | 1×107         | ±1.5            | 1.6×106                            | <10         | <10   | 109           | Yes      |
|                            | TiN/As:SiO2/TiN106                                        | ~1×104        | ±3              | 2.5×107                            | 23          | 52    | 105           | No       |
| Metal–insulator transition | Pt/VO2/Pt107                                              | 50            | ±0.4            | >106                               | <20         | NR    | NR            | Yes      |
|                            | TiN/NbO2/TiN108                                           | 1×103         | ±1              | 8×108                              | NR          | NR    | 103           | Yes      |
|                            | TiN/NbO2/W109                                             | 1×103         | ±1              | 1×107                              | 103         | NR    | 106           | Yes      |
| Charge injection           | TiN/AsTeGeSiN/TiN110                                      | 1×103         | ±1.5            | 1.1×107                            | 103         | NR    | 108           | Yes      |
| Filamentary                | W/Ag/MgO/Ag/W*111                                         | 1×107         | ±1              | 4                                  | 50          | 25    | 104 (d.c.)    | Yes      |
|                            | Pt/Ag:SiOxNy/Pt33                                         | 1×105         | ±0.3            | 1                                  | 3×106       | 3×107 | 106           | Yes      |
|                            | Pd/Ag/HfO2/Ag/Pd*92                                       | 1×1010        | ±0.4            | 4                                  | 75          | 250   | 108           | Yes      |
| Field assisted             | Not reported (NR)112                                      | 1×1010        | ±1              | 5×106                              | <50         | <10   | 108           | Yes      |
| Rectifying selectors       |                                                           |               |                 |                                    |             |       |               |          |
| Si PN junction             | P2+/N+/N2+ epitaxial Si*113                               | 1011          | ±2              | NR                                 | NR          | NR    | NR            | No       |
| Schottky barrier           | Al/Si*114                                                 | 105           | ±2.6            | >10                                | NR          | NR    | NR            | No       |
| Self-nonlinear memristors  |                                                           |               |                 |                                    |             |       |               |          |
| Tunneling                  | TiN/Al2O3/TiO2/TiN*115                                    | 100           | 3               | >105                               | 10          | 10    | NR            | Yes      |
| Metal-insulator-transition | W/VOx/Pt*116                                              | >10           | ±0.5            | >2×106                             | <104        | <104  | 100           | Yes      |
| Self-rectifying memristors |                                                           |               |                 |                                    |             |       |               |          |
| Si PN junction             | p-Si/SiO2/n-Si54                                          | 105           | ±1.5            | >4×104                             | NR          | NR    | >100 (d.c.)   | No       |
| Schottky barrier           | Ni/HfO2/n-Si*117                                          | 103           | ±1              | >10                                | NR          | NR    | NR            | No       |
|                            | Cu/HfO2/n-Si118                                           | 104           | ±1              | >50                                | NR          | NR    | NR            | No       |
|                            | Ag/a-Si/p-poly-Si119                                      | 106           | ±0.5            | >2.5×103                           | <100        | <100  | >108          | No       |

a Defined as the read voltage (*V*r) at which the non-linearity is calculated. b Most literature only reports the ON switch speed, but both ON and OFF switch speeds of a selector are critical to the array operation (*t*off for a bidirectional selector usually is the relaxation time). For self-selective memristors, speed is the programming time that depends on the voltage amplitude, which is likely much longer than the read time. c Ratios between *I*Vr and *I*1/3Vr (instead of *I*1/2Vr for all other nonlinear selectors and *I*–Vr for rectifying selectors in the rest of the table). Best reported parameters in each category are highlighted in bold fonts. The last two types of device in this table are both self-selective memristors, meaning no external selectors are needed in the cell. The bidirectional ones are called self-nonlinear memristors, to distinguish them from the unipolar ones that are self-rectifying. NR, not reported. Devices marked with \* require electroforming.

performs inference of large pre-trained networks at low power without on-chip learning capability. Intel's Loihi, built upon 14 nm FinFET technology, features on-chip learning capability with 128 neuromorphic cores supporting up to 128 thousand neurons and 128 million single-bit synapses. In addition to hardware simulation, the SpiNNaker neuromorphic supercomputer provides a digital circuit-based software simulation of neural network[s61.](#page-12-51) With up to a million ARM 968 processor cores, powerful on-board routers and shared SRAM for synaptic weight storage, the SpiNNaker alleviates the von Neumann bottleneck through its improved parallelism with a peak power of 75 kW.

Nevertheless, because the CMOS devices were not created or optimized for the purposes of neuromorphic computing, they do not faithfully resemble synapses and hence lack the intrinsic hardware learning capability. Consequently, those silicon synapses and neurons require complex circuits based on transistors, which are

difficult to scale or stack. Bulky memory and frequent memory visits limit the learning capability as well as energy and area efficiency in these systems. Therefore, compared with the 10 μm2 neuron area, 0.001 μm2 synaptic area and ~2 fJ synaptic operation energy in biointelligent systems, the CMOS-based SNNs are 20 times, 400 times and 2,000 times as large, respectivel[y63.](#page-12-53)

**Memristor SNNs.** Memristors can emulate synapses and even neurons more faithfully because they share fundamentally similar functioning mechanisms to their bio-counterparts: both mechanisms are closely associated with ion migration. Because the capability of a neural network is generally determined by its size (how deep the network is and how many neurons per layer), a more powerful and yet energy-efficient system could be built with the more scalable and stackable devices. Memristors have been studied for simulating both the synaptic plasticity[64–](#page-13-33)[68](#page-13-34) and neural integrate-and-fir[e31,](#page-12-23)[69–](#page-13-35)[72](#page-13-36), implementing interesting learning rules observed in their bio-counterparts. Memristors can be used as the physical weights not only for DNNs but also SNNs. In particular, memristor SNNs could be used to achieve bio-realistic unsupervised learning capability[73–](#page-13-37)[75.](#page-13-38)

Because SNNs process data as time-series events, a critical requirement on the synapses and neurons is their dynamics. Memristive devices exhibit a delay time in their ON switching, which ranges from less than 100 ps to tens of seconds, depending on the materials of the device and the parameters of the applied electrical pulses such as the pulse amplitude, duration and interval. This delay dynamic can be explored as the leaky integrateand-fire (LIF) function of a neuron. If the memristor is also volatile, it relaxes back to its OFF states under zero electrical bias after ON switching. The neuron based on such a memristor goes back to its resting state after firing without the need of a reset operation, just like a biological neuron. The relaxation dynamics of such volatile memristors can also be used to emulate critical synaptic dynamics of a biological synapse.

Recently, memristive devices with different types of relaxation dynamics, such as second-order thermal dissipation dynamic[s76,](#page-13-39) phase-growth dynamics (for example chalcogenides[\)32,](#page-12-54) and phasetransition dynamics (for instance Mott materials)[77](#page-13-40) have been used to bio-realistically implement the temporal plasticity of chemical synapses. Figure [3](#page-8-0) summarizes the device structure (left), dynamics (middle) and synaptic and neuronal behaviour (STDP and LIF, right) of these devices. Compared with previous memristive synapses without the relaxation dynamics, the plasticity exhibited by these synapses is one step closer to that of actual chemical synapses in that the synaptic weight tuning does not need the overlapping of the pre- and postsynaptic waveforms. However, the origin of relaxation dynamics in these devices is not ion diffusion, an important difference from the physical processes of chemical synapses, which may limit the fidelity and variety of desired synaptic functions. This issue could be addressed by introducing diffusive memristors with their unique dynamics closely resembling that of the ionic channels of chemical synapses[33](#page-12-24)[,65.](#page-13-41) Diffusive memristors are a newly developed, volatile, bidirectional threshold switch with fast diffusive ions (for example Ag and Cu) in dielectrics (such as SiO*x* and HfO2). The biggest difference of the diffusive memristor from the traditional memristors is that once the voltage across the terminals is removed, it automatically goes back to its original high-resistance state. Depending on the material systems (dielectric and dopants), the times required to switch on can vary by orders of magnitude, as can the times to relax back. The diffusion dynamics in a diffusive memristor offers similar physical behaviour to the biological Ca2+ dynamics, which could faithfully emulate a variety of temporal synaptic and neuronal properties. By physically integrating the diffusive memristor with a non-volatile memristor in series, both short-term and long-term plasticity have been demonstrated in a combined synapse[33.](#page-12-24) A small-scale integrated memristive SNN with unsupervised learning capability was built with 24 (3 × 8) 1T1R synapses and three memristive neurons (Fig. [4a\)](#page-9-0). The synapses are based on Ta/HfO2/Pd memristors (Fig. [4b,c](#page-9-0)), and the neurons are built with Pt/Ag:SiO2/Pt diffusive memristors (Fig. [4d,e](#page-9-0)). The fully integrated all-memristor SNN is suitable to scale and stack, and could lead to future ultra-large-scale networks.

**Capacitive neural networks.** In addition to resistive neural networks, in which conductance represents the synaptic weight, other electrical variables, such as the capacitance of synaptic elements, can also act as the weight of the network. Networks built on capacitive synapses feature a lower static power and potentially better emulation of neural dynamics, providing an alternative physical embodiment of biological neural functionality. Capacitors can conveniently integrate discrete spiking signals with leaky dynamics, making such capacitive neural networks a natural choice for SNNs. Memcapacitors, instead of memristors, are needed to build capacitive synapses and capacitive neurons to enable such networks. As another type of memristive device, the memcapacitor possesses a capacitance that depends on its bias histor[y78,](#page-13-42) which has been far less experimentally pursued than memristors. Nevertheless, a neuro-transistor was developed recently by integrating dynamic pseudo-memcapacitors as the gates of transistors to produce electronic analogues of the soma and axon of a neuron with LIF dynamics augmented by signal gain on the outpu[t22.](#page-12-17) It should be noted that the artificial neurons need to be able to propagate signals in a similar way to actual neurons once multiple layers exist in the network. Paired with non-volatile pseudomemcapacitive synapses, a Hebbian-like learning mechanism is implemented in a capacitive switching network, thus leading to observed associative learning. A prototypical fully integrated capacitive neural network has been built to show the weighted sum capability of the capacitive crossbars. Such integrated arrays are still at very small scale with 16 capacitive synapses and four capacitive neurons (Fig. [4f–m\)](#page-9-0). Increased research efforts are needed to develop memcapacitive devices to enable larger capacitive neural networks. The requirements for these new devices include high capacitance at small size, analogue programmability with both long- and short-term retention, electroforming-free, low variability, reasonably high endurance and speed, good stackability and CMOS compatibility.

Even though encouraging results have been achieved recently in engineering critical dynamics into traditional memristors for SNNs and demonstrating Hebbian-like learning, there is still a lot of material and device work to be done to develop other key functionalities, such as gradual and symmetric weight update in the synapse with each spike, signal propagation without using transistors in the neuron, memcapacitive devices and so on. In addition, algorithm is another key issue that limits the progress of SNN development. STDP is a useful learning rule of synaptic plasticity, but most of the research on this aspect is limited to mimicking the biological STDP phenomenon with emerging devices. There are a few SNN modelling studies that use STDP to update the synaptic weight[s79,](#page-13-43) but no experimental demonstration of training a largescale SNN using STDP has been reported so far. Efficient algorithms (such as the backpropagation for DNNs) must be invented to bring the training of SNNs from the current device level to a large array level.

### **Array integration and upscaling**

Building large arrays involves fabrication and integration techniques, of which those readily available in a standard semiconductor foundry are preferred. To pattern the electrode wires, direct writing tools such as electron beam lithography have excellent resolution, but they are a sequential process best suited for prototyping single or

![](_page_8_Figure_2.jpeg)

**Corrected**

<span id="page-8-0"></span>**Fig. 3 | Memristive synapses and neurons. a**, Schematic of a diffusive memristor with Ag-doped dielectrics. **b**, Ag dynamics in a diffusive memristor which emulate the Ca2+ dynamics. Ag diffuses into the gap region between Ag nanoclusters under an electric field, forming a Ag filament. Once the voltage bias is removed, the Ag is removed by the interfacial energy or mechanical stress, leading to filament fracture. **c**, Experimental demonstration of spike-timing-dependent plasticity with the diffusive memristor-based artificial synapse. The plot shows the conductance (weight) change of the synapse with variation in spike-timing, similar to that of the chemical synapse[33](#page-12-24). **d**, The schematic of a second-order Pd/Ta2O5–*x*/TaOy/Pd memristor with a reduced TaO*y* conductive filament (CF). **e**, Dynamics of the two state-variables (size and temperature of the conduction channel) of the second-order memristor. **f**, Experimental demonstration of spike-timing-dependent plasticity with the second-order memristor-based artificial synapse using thermal dissipation dynamics[76.](#page-13-39) Symbols, measured data; lines, numerical model. **g**, Schematic of a phase-change memristor. TE, top electrode; BE, bottom electrode. **h**, Phase-growth dynamics upon electrical stimulus, which are functionally equivalent to the postsynaptic potentials (PSPs) of a neural cell. **i**, Experimental demonstration of integrate-and-fire to various input spikes with a phase-change memristor neuron[32](#page-12-54). ampl., amplitude. **j**, Illustration of filament in a VO2 Mott insulator memristor. *V*IMT is the threshold switching voltage across the two terminals of the device. **k**, Dynamics of the stochastic nucleation phase transition of the Mott insulator. **l**, Experimental demonstration of various spiking probabilities of the Mott insulator memristor-based artificial neuron with different external electrical configuration[s77](#page-13-40). *V*DD is the voltage supplied to the Mott insulator connected to the drain of a series transistor (not shown here), while the voltage out of the integrator (inset), *V*integrator, is applied to the gate of the series transistor. Reproduced from ref. [76,](#page-13-39) American Chemical Society (**f**); ref. [32,](#page-12-54) Springer Nature Ltd. (**h**,**i**). Adapted from ref. [74](#page-13-44), Springer Nature Ltd. (**a**); ref. [33,](#page-12-24) Springer Nature Ltd. (**b**,**c**); ref. [76,](#page-13-39) American Chemical Society (**d**,**e**); ref. [32](#page-12-54), Springer Nature Ltd. (**g**); and ref. [77](#page-13-40), IEEE (**j**–**l**).

**Fully memristive neural network on chip**

![](_page_9_Figure_3.jpeg)

### **Fully capacitive neural network on chip**

![](_page_9_Figure_5.jpeg)

<span id="page-9-0"></span>**Fig. 4 | Spiking neural networks with integrated synapses and neurons fully based on memristive or memcapacitive devices. a**, Optical micrograph of a fully memristive neural network on chip, consisting of an 8 × 8 1T1R memristive synaptic crossbar integrated with eight diffusive memristor artificial neurons. (Each neuron used in this demonstration has an external capacitor not shown here.) **b**, Scanning electron micrograph of a single 1T1R cell. Memristive synapses of the same row share bottom electrode lines, while those of the same column share top electrode and transistor gate lines. **c**, Cross-sectional transmission electron microscopy image of the integrated Pd/HfO2/Ta drift memristor prepared by focused-ion-beam cutting. **d**, Scanning electron micrograph of a single diffusive memristor junction. **e**, High-resolution transmission electron micrograph of the cross-section of the Pt/Ag/SiO2:Ag/Ag/Pt diffusive memristor, showing amorphous background SiO2 with nanocrystalline thin Ag layers. **f**, Scanning electron micrographs of a fully capacitive neural network consisting of a 4 × 4 crossbar array of synapses (blue box) integrated with four neurons (red box). **g**, Each volatile capacitive switching neuro-transistor was interfaced to four synapses. **h**,**i**, The capacitive switching neurons and synapses were both built with Ag-based memristors (magenta and cyan boxes, respectively) in series with integrated capacitors (red and blue boxes). **j**, Structural analysis of the neuro-transistor with the cross-sectional transmission electron micrograph shows the diffusive memristor sitting on the gate of a convention n-MOSFET. **k**, High-resolution transmission electron micrograph of the diffusive memristor showing the amorphous SiO*x* dielectric matrix with nanocrystalline thin Ag layers. **l**, Structural analysis of the capacitive switching synapse. The transmission electron micrograph shows the electrochemical metallization cell on top of an HfO2 capacitor. **m**, The zoomed high-resolution transmission electron micrograph of the electrochemical metallization cell shows the thick Ag electrode responsible for the longer retention time of the low-conductance state of the memristo[r22](#page-12-17)[,74.](#page-13-44) Reproduced from ref. [74,](#page-13-44) SNL (**a**–**e**); and ref. [22,](#page-12-17) SNL (**f**–**m**).

discrete nanodevices. They are unfortunately not the best tools for array-level fabrication of memristor crossbars because of their low throughput. Nanoimprint lithography has been successfully demonstrated for array-level integration with sub-10-nm feature siz[e80](#page-13-45)[,81,](#page-13-46) but it has not been commissioned in mainstream integrated circuit foundries partially because of concerns over defectivity and imprint

### **Box 1 | Brain-inspired computing**

Brain-inspired computing, a part of the broadly defned area of artifcial intelligence, aims to solve problems through similar principles to those used by the brain. Even though our understanding of the human brain is limited, we know that a brain has the ability to learn, to solve tasks from experience and to process analogue information directly. Te brain processes information in the location where it is stored without physically separating the memory and computing functions (so it is an 'in-memory' computing system). Tere are two main subfelds. In spiking neural networks (SNNs), information is encoded and conveyed based on the temporal relationship between diferent pulses. Te computing activities are triggered by events. Te second main brain-inspired computing paradigm is non-spiking neural networks, which normally do not encode data into the timing of the signals.

Neural networks comprise layers of artifcial 'neurons' that receive, process and transmit signals, and 'synapses' that connect the neurons and evolve to alter the connection patterns during learning. A simple neural network topography is schematically illustrated in panel **a**. Te neurons (circles) in the input layer receive signals and propagate them to the neurons in the hidden layers (whose values are invisible during the computation). Te weighted sum (∑) is processed through a nonlinear activation function in the hidden layer—a sigmoid is shown here as an example—and then propagated to the output layer. Te synapses are connections of diferent strength (or weight *wij*, shown as arrows of various thickness) that are tunable during training. Te complexity of a neural network is ofen characterized by the number of units or connections. Historically, a neural network with more than one hidden layer is referred to as a deep neural network (DNN), which is the computational framework to implement deep learning algorithms. However, the concept of 'deep' has been evolving over time. For example, networks with fewer than ten layers in computer vision are regarded as 'shallow' nowadays. It is also worth noting that, although traditionally in the literature DNNs are mainly non-spiking neural networks, spiking neuron models are starting to be adopted in DNNs.

To enable a computation function (ofen referred to as inference), the weights of the synapses in the neural network are modulated to minimize a loss function that measures the error between the output of the network and known targets (for supervised learning)—a process called 'training'. Although efcient algorithms for training SNNs are generally still lacking, training DNNs has been widely practised and mainly based on an optimization algorithm called 'gradient descent'. Tis algorithm takes derivatives of a loss function, measures the change in error caused by a change in the weight (ofen referred to as a 'backpropagation' process) and chooses parameters in the next iteration that lead to a smaller error.

If the optimized synaptic weights are loaded into the neural network from another source where the training takes place, the process is called ofine training (or ex situ learning). On the other hand, if the synaptic weights are adjusted during training in the same network that is used for inference, this is called online training (or in situ learning). In the case where all the training samples are labelled because their corresponding outputs from the network are known, it is called supervised learning; otherwise it is unsupervised learning. In addition to these two, there is reinforcement learning, which is neither supervised nor unsupervised. Reinforcement learning self-adjusts its decisionmaking process based on the early experiences to maximize the collective rewards of a specifc problem, such as a Go game.

Whereas the learning capability of the brain is successfully emulated in CMOS-based hardware, the 'in-memory computing' feature is less developed, partially because most CMOS neuromorphic hardware has been built upon the traditional von Neumann architecture with separated memory units to store intermediate computing results. With non-volatile emerging devices, in-memory computing becomes realistic. Take vectormatrix multiplication, the core computing operation of many deep learning algorithms as an example (panel **b**): here, computing and storing take place in the same location in a crossbar array. Input signals (voltage vector in this case) are delivered to each junction of diferent conductance (diferent shades of orange), where a current as the multiplication result is generated according to Ohm's law (*I* = *VG*, where *G* is conductance). Te current at each column follows Kirchhof 's current law, which allows one to obtain the summation of the multiplications (currents of each junction) by measuring the total current out of each column (*Ij* = ∑*<sup>i</sup>* = ∑*<sup>i</sup> Vi Gi,j*). In a memristive neural network, the conductance of a cell can be continuously tuned, and the value is stored in the same cell. Because synaptic weights can be negative and the conductance of a device is always positive, a popular technique is to represent weights as the conductance diference between two devices (a diferential pair).

![](_page_10_Figure_9.jpeg)

![](_page_10_Figure_10.jpeg)

mould durability. It is likely that some types of photolithography techniques, such as deep ultraviolet (193 nm wavelength) lithography with resolution enhancement techniques, or extreme ultraviolet (13.5 nm wavelength) lithography, will be the choices of patterning for mass production of massive memristor crossbar arrays in the IC industry. Most physical vapour deposition (PVD) techniques, such as evaporation and sputtering, are adopted for the deposition of electrode and switching materials. However, depending

on the choice of material, PVD derivatives and other techniques may be needed. For example, titanium nitride, a standard electrode material in the IC industry, is made by reactive sputtering in a high nitrogen flow rate. But copper, which has low resistance in the interconnection, is best fabricated using electroplating. Atomic layer deposition, a variant of chemical vapour deposition, is becoming popular in making the switching layer because of the high conformity, thickness uniformity and accurate control, which lead to excellent electrical properties of the resultant films. Atomic layer deposition also offers the ability to tune the film composition by allowing the choice of precursors and controlling the number of cycles for each precursor.

To accommodate a higher dimension of input data in more complex problems, it is necessary to expand the size of the memristor neural networks. Large array operation imposes more stringent requirements on the array parameters. For instance, the wire resistance from both the memristor electrodes and the transistor would distort the signal propagation (voltage drop along the wire), increase the power consumption within the interconnections and limit the computing precision. To mitigate this effect, the ratio of wire to device resistance needs to remain low. One solution is to adopt advanced interconnection technology such as copper damascene for both transistors and memristors. Equally important is to compensate for the wire effects at the algorithm level if there is limited room, from a technological point of view, to achieve small wire resistanc[e14](#page-12-11),[18.](#page-12-55)

Alternatively, the wire resistance challenge can be addressed by using multiple smaller arrays organized into different layers, either in lateral organization (2D) or vertically (3D). Increasing the depth of a neural network (number of layers) is a standard practice to enhance the performance (such as classification accuracy). From a hardware perspective, this practice will greatly relax the stringent demand of low wire resistance.

One interesting direction is to build 3D networks, in which each individual layer could serve different functionalities such as sensors, data processing or storage units[82.](#page-13-47) The peripheral circuits for the 3D passive arrays need to be smartly designed so that efficient data flow between each layer is implemented. From a fabrication or manufacturing standpoint, alignment requirements for a layer by layer stacking approach might be a major roadblock for high-throughput manufacturing. Technologies currently used in other fields (such as string stacking for 3D NAND flash) may be adopted to make 3D memristive neural networks[83](#page-13-48).

## **Conclusions**

Memristive neural networks address the speed and energy efficiency issues in computing hardware from different perspectives. On the device level, the energy required during the computation and weight update is minimal. On the architecture level, computation is performed on-site where information is stored, avoiding data movement as encountered in traditional digital computers. Directly using physical laws, the computation can be accomplished very efficiently with massive parallelism, regardless of the array size, leading to a considerable improvement in computing throughput. The capability of directly processing analogue information from sensors makes it possible to futher reduce the time and energy consumption.

Experimental implementation of large memristive neural networks with applications in real-world datasets, however, is in its early developmental stage compared with CMOS counterparts. There remain several technical challenges for the construction and operation of both active and passive arrays. On the device level, memristive switching is generally based on formation and rupture of conducting filaments, and the electron and ion transport processes are inherently stochastic. As a result, the variability in the device conductance, especially for high-resistance regimes that are favoured for their lower power consumption, is an intrinsic problem that needs attention. The stochasticity of the memristive devices could be exploited to better mimic the neuron behaviours but needs to be taken into account during synaptic learning. The lack of an ideal selector poses a major challenge to the realization of large and dense crossbar arrays because of sneak path current and half-select issues. Wire resistance in ultra-large networks also adversely affects the programming accuracy of synaptic weights during training. The nonlinearity in *I–V* behaviour, and the asymmetry in potentiation and depression, all impose challenges in array operation. We have proposed some plausible solutions to these issues in this Review, and we believe that the device property requirements should be considered based on different applications and algorithms.

For memristive neural networks to be adopted for mainstream machine learning applications, devices and arrays will need to be mass-produced, making use of the current infrastructure in the IC industry. At the early stage of device development, various materials were used to demonstrate the memristor behaviour and/or to engineer certain properties of the device. More in-depth studies are needed to better understand the microscopic picture and mechanisms of switching, so that we can narrow down the material selection and focus on those that are available to or accepted by the mainstream IC industry. This practice will aid the manufacturing and adoption of memristive devices using existing technolog[y84.](#page-13-49) Equally important is the market potential of these devices and neural networks. As computer hardware is difficult and expensive to build, it should find a niche application that many people will need, or be reconfigurable and versatile for many applications that have a sizeable market.

Most of the current peripheral circuits for memristive neural networks are based on existing CMOS technology. Heterogeneous integration with the memristive crossbar, active or passive, becomes critical. This emphasizes the importance of materials and process compatibility of memristor technology with the existing IC infrastructure. A fully integrated memristive neural network system will have much lower power consumption and shorter latency, even with transistors at a reasonably small technology node. Of particular importance in the peripheral circuits are ADCs and DACs. Memristors are capable of interfacing with analogue data collected from sensors, as has been demonstrated recentl[y85](#page-13-50). However, most of the computing engines need software, digital peripheral circuits and a digital computer that controls all of the communications. A mixed signal system will require low-power ADCs and DACs. While CMOS-based ADCs and DACs are saturated in power efficiency, there is a great need to develop new technologies for these components with much better power efficiency. Emerging devices, such as memristors, may also be used to implement these functionalitie[s86](#page-13-51).

Last but not least, developing a new computing hardware system requires coherently organized collaborative research across different disciplines (engineering, biology, physics and others) and at different levels (algorithm, architecture, system, circuits, devices/materials). Most materials and device efforts have been focusing on tailoring device properties to fit existing computing algorithms, but this readily trivializes or excludes some intriguing new device properties. With better understanding of the brain, new algorithms should be developed that will promote hardware development as well. On the other hand, there has been rare effort in algorithm and array architecture design or modification to use these new discoveries in materials and devices. This represents opportunities for both communities. Scientists and engineers with different expertise may learn from the success of von Neumann architecture, modularly engineering each portion and integrating them into a complex computing system. Eventually, co-design from both low

(materials/devices) and high (array architecture and algorithm) ends would greatly advance this last computing frontier—neuromorphic computing.

Received: 20 September 2018; Accepted: 16 February 2019; Published online: 20 March 2019

### **References**

- <span id="page-12-0"></span>1. Moore, G. E. Cramming more components onto integrated circuits. *Electron. Mag* **38**, 114–117 (1965).
- <span id="page-12-1"></span>2. von Neumann, J. First draf of a report on the EDVAC. *IEEE Ann. Hist. Comput.* **15**, 27–75 (1993).
- **Tis is an exact copy of the original typescript draf written by von Neumann in 1945, with typographical errors corrected**.
- <span id="page-12-2"></span>3. *NVIDIA Launches the World's First Graphics Processing Unit: GeForce 256* [http://www.nvidia.com/object/IO\\_20020111\\_5424.html](http://www.nvidia.com/object/IO_20020111_5424.html) (NVIDIA, accessed 30 July 2018).
- <span id="page-12-3"></span>4. Jouppi, N. P. et al. In-datacenter performance analysis of a tensor processing unit. In *44th Int. Symp. Computer Architecture (ISCA)* 1–12 (ACM, 2017).
- <span id="page-12-4"></span>5. Kautz, W. H. Cellular logic-in-memory arrays. *IEEE Trans*. *Comput. C* **18**, 719–727 (1969).
- <span id="page-12-5"></span>6. Wolf, S. A. et al. Spintronics: a spin-based electronics vision for the future. *Science* **294**, 1488–1495 (2001).
- 7. Ovshinsky, S. R. Reversible electrical switching phenomena in disordered structures. *Phys. Rev. Lett.* **21**, 1450–1453 (1968).
- 8. Yang, J. J. et al. Memristive switching mechanism for metal/oxide/metal nanodevices. *Nat. Nanotechnol.* **3**, 429–433 (2008).
- <span id="page-12-6"></span>9. Mikolajick, T. et al. FeRAM technology for high density applications. *Microelectron. Reliab.* **41**, 947–950 (2001).
- <span id="page-12-7"></span>10. Chua, L. Memristor—the missing circuit element. *IEEE Trans. Circuit Teory* **18**, 507–519 (1971).
- <span id="page-12-8"></span>11. Strukov, D. B., Snider, G. S., Stewart, D. R. & Williams, R. S. Te missing memristor found. *Nature* **453**, 80–83 (2008).
- <span id="page-12-9"></span>12. Pi, S. et al. Memristor crossbar arrays with 6-nm half-pitch and 2-nm critical dimension. *Nat. Nanotechnol.* **14**, 35–39 (2019).
- <span id="page-12-10"></span>13. Choi, B. J. et al. High-speed and low-energy nitride memristors. *Adv. Funct. Mater.* **26**, 5290–5296 (2016).
- <span id="page-12-11"></span>14. Hu, M., Strachan, J. P., Li, Z. & Williams, S. R. Dot-product engine as computing memory to accelerate machine learning algorithms. In *17th Int. Symp. Quality Electronic Design (ISQED)* 374–379 (IEEE, 2016).
- <span id="page-12-12"></span>15. Yao, P. et al. Face classifcation using electronic synapses. *Nat. Commun.* **8**, 15199 (2017).
- <span id="page-12-13"></span>16. Li, C. et al. Analogue signal and image processing with large memristor crossbars. *Nat. Electron.* **1**, 52–59 (2018).
- <span id="page-12-34"></span>17. Li, C. et al. Efcient and self-adaptive in-situ learning in multilayer memristor neural networks. *Nat. Commun.* **9**, 2385 (2018).
- <span id="page-12-55"></span>18. Li, C. et al. Long short-term memory networks in memristor crossbar arrays. *Nat. Mach. Intell.* **1**, 49–57 (2019).
- <span id="page-12-14"></span>19. Hu, M. et al. Memristor-based analog computation and neural network classifcation with a dot product engine. *Adv. Mater.* **30**, 1705914 (2018).
- <span id="page-12-15"></span>20. Prezioso, M. et al. Training and operation of an integrated neuromorphic network based on metal–oxide memristors. *Nature* **521**, 61–64 (2015).
- <span id="page-12-16"></span>21. Sheridan, P. M. et al. Sparse coding with memristor networks. *Nat. Nanotechnol.* **12**, 784–789 (2017).
- <span id="page-12-17"></span>22. Wang, Z. et al. Capacitive neural network with neuro-transistors. *Nat. Commun.* **9**, 3208 (2018).
- <span id="page-12-18"></span>23. Waser, R. & Aono, M. Nanoionics-based resistive switching memories. *Nat. Mater.* **6**, 833–840 (2007).
- 24. Valov, I., Waser, R., Jameson, J. R. & Kozicki, M. N. Electrochemical metallization memories—fundamentals, applications, prospects. *Nanotechnology* **22**, 254003 (2011).
- 25. Yang, J. J., Strukov, D. B. & Stewart, D. R. Memristive devices for computing. *Nat. Nanotechnol.* **8**, 13–24 (2013).
- 26. Jeong, D. S., Kim, K. M., Kim, S., Choi, B. J. & Hwang, C. S. Memristors for energy‐efcient new computing paradigms. *Adv. Electron. Mater* **2**, 1600090 (2016).
- 27. Burr, G. W. et al. Neuromorphic computing using non-volatile memory. *Adv. Phys. X* **2**, 89–124 (2017).
- 28. Lee, J. & Lu, W. D. On‐demand reconfguration of nanomaterials: when electronics meets ionics. *Adv. Mater.* **30**, 1702770 (2018).
- <span id="page-12-19"></span>29. Yang, Y. & Huang, R. Probing memristive switching in nanoionic devices. *Nat. Electron.* **1**, 274–287 (2018).
- <span id="page-12-20"></span>30. Silver, D. et al. Mastering the game of Go with deep neural networks and tree search. *Nature* **529**, 484–489 (2016).
- <span id="page-12-23"></span>31. Pickett, M. D., Medeiros-Ribeiro, G. & Williams, R. S. A scalable neuristor built with Mott memristors. *Nat. Mater.* **12**, 114–117 (2013).
- <span id="page-12-54"></span>32. Tuma, T., Pantazi, A., Le Gallo, M., Sebastian, A. & Elefheriou, E. Stochastic phase-change neurons. *Nat. Nanotechnol.* **11**, 693–699 (2016).
- <span id="page-12-24"></span>33. Wang, Z. et al. Memristors with difusive dynamics as synaptic emulators for neuromorphic computing. *Nat. Mater.* **16**, 101–108 (2017).
- <span id="page-12-25"></span>34. Yang, J. J. et al. High switching endurance in TaO*x* memristive devices. *Appl. Phys. Lett.* **97**, 232102 (2010).
- <span id="page-12-26"></span>35. Wright, C. D., Hosseini, P. & Vazquez Diosdado, J. A. Beyond von-Neumann computing with nanoscale phase-change memory devices. *Adv. Funct. Mater.* **23**, 2248–2254 (2013).
- <span id="page-12-22"></span>36. Jiang, H. et al. Sub-10 nm Ta channel responsible for superior performance of a HfO2 memristor. *Sci. Rep.* **6**, 28525 (2016).
- <span id="page-12-27"></span>37. Choi, S. et al. SiGe epitaxial memory for neuromorphic computing with reproducible high performance based on engineered dislocations. *Nat. Mater.* **17**, 335–340 (2018).
- <span id="page-12-21"></span>38. Chen, J.-Y. et al. Dynamic evolution of conducting nanoflament in resistive switching memories. *Nano Lett.* **13**, 3671–3677 (2013).
- <span id="page-12-28"></span>39. van de Burgt, Y. et al. A non-volatile organic electrochemical device as a low-voltage artifcial synapse for neuromorphic computing. *Nat. Mater.* **16**, 414–418 (2017).
- <span id="page-12-29"></span>40. Boniardi, M. & Ielmini, D. Physical origin of the resistance drif exponent in amorphous phase change materials. *Appl. Phys. Lett.* **98**, 243506 (2011).
- <span id="page-12-30"></span>41. Yi, W. et al. Quantized conductance coincides with state instability and excess noise in tantalum oxide memristors. *Nat. Commun.* **7**, 11142 (2016).
- <span id="page-12-31"></span>42. Lastras-Montaño, M. A. & Cheng, K. T. Resistive random-access memory based on ratioed memristors. *Nat. Electron.* **1**, 466–472 (2018).
- <span id="page-12-32"></span>43. Alibart, F., Gao, L., Hoskins, B. D. & Strukov, D. B. High precision tuning of state for memristive devices by adaptable variation-tolerant algorithm. *Nanotechnology* **23**, 075201 (2012).
- <span id="page-12-33"></span>44. Ambrogio, S. et al. Equivalent-accuracy accelerated neural-network training using analogue memory. *Nature* **558**, 60–67 (2018).
- <span id="page-12-35"></span>45. Shafee, A. et al. ISAAC: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars. in *2016 ACM/IEEE 43rd Int. Symp. Comp. Archit. (ISCA)* 14–26 (IEEE, 2016).
- <span id="page-12-36"></span>46. Jiang, H. et al. Pulse-width modulation based dot-product engine for neuromorphic computing system using memristor crossbar array. In *2018 IEEE Int. Symp. Circuits and Systems (ISCAS)* [https://doi.org/10.1109/](https://doi.org/10.1109/ISCAS.2018.8351276) [ISCAS.2018.8351276](https://doi.org/10.1109/ISCAS.2018.8351276) (IEEE, 2018).
- <span id="page-12-37"></span>47. Burr, G. W. et al. Experimental demonstration and tolerancing of a large-scale neural network (165 000 synapses) using phase-change memory as the synaptic weight element. *IEEE Trans. Electron Devices* **62**, 3498–3507 (2015).
- <span id="page-12-38"></span>48. Woo, J. & Yu, S. Resistive memory-based analog synapse: the pursuit for linear and symmetric weight update. *IEEE Nanotechnol. Mag.* **12**, 36–44 (2018).
- <span id="page-12-39"></span>49. Gokmen, T. & Vlasov, Y. Acceleration of deep neural network training with resistive cross-point devices: design considerations. *Front. Neurosci.* **10**, 33 (2016).
- <span id="page-12-40"></span>50. Suri, M. et al. Phase change memory as synapse for ultra-dense neuromorphic systems: application to complex visual pattern extraction. In *2011 Int. Electron Devices Meeting (IEDM)* 4.4.1–4.4.4 (IEEE, 2011).
- <span id="page-12-41"></span>51. Hsu, C.-W. et al. 3D vertical TaO*x*/TiO2 RRAM with over 103 self-rectifying ratio and sub-µA operating current. In *2013 Int. Electron Devices Meeting (IEDM)* 10.4.1–10.4.4 (IEEE, 2013).
- <span id="page-12-42"></span>52. Jang, J.-W., Park, S., Burr, G. W., Hwang, H. & Jeong, Y.-H. Optimization of conductance change in Pr(1−*<sup>x</sup>*)Ca*x*MnO3-based synaptic devices for neuromorphic systems. *IEEE Electron Device Lett.* **36**, 457–459 (2015).
- <span id="page-12-43"></span>53. Merrikh-Bayat, F. et al. High-performance mixed-signal neurocomputing with nanoscale foating-gate memory cell arrays. *IEEE Trans. Neural Netw. Learn. Syst.* **29**, 4782–4790 (2018).
- <span id="page-12-44"></span>54. Li, C. et al. Tree-dimensional crossbar arrays of self-rectifying Si/SiO2/Si memristors. *Nat. Commun.* **8**, 15666 (2017).
- <span id="page-12-45"></span>55. Woo, J., Peng, X., & Yu, S. Design considerations of selector device in cross-point RRAM array for neuromorphic computing. In *2018 IEEE Int. Symp. Circuits and Systems (ISCAS)* [https://doi.org/10.1109/](https://doi.org/10.1109/ISCAS.2018.8351735) [ISCAS.2018.8351735](https://doi.org/10.1109/ISCAS.2018.8351735) (IEEE, 2018).
- <span id="page-12-46"></span>56. Wang, W. et al. Learning of spatiotemporal patterns in a spiking neural network with resistive switching synapses. *Sci. Adv.* **4**, eaat4752 (2018).
- <span id="page-12-47"></span>57. Hebb, D. O. *Te Organization of Behavior*. (Wiley, New York, 1949).
- <span id="page-12-48"></span>58. Sourikopoulos, I. et al. A 4-fJ/spike artifcial neuron in 65 nm CMOS technology. *Front. Neurosci.* **11**, 123 (2017).
- <span id="page-12-49"></span>59. Indiveri, G., Chicca, E. & Douglas, R. J. A VLSI array of low-power spiking neurons and bistable synapses with spike-timing dependent plasticity. *IEEE Trans. Neur. Netw.* **17**, 211–221 (2006).
- <span id="page-12-50"></span>60. Benjamin, B. V. et al. Neurogrid: a mixed-analog-digital multichip system for large-scale neural simulations. *Proc. IEEE* **102**, 699–716 (2014).
- <span id="page-12-51"></span>61. Furber, S. B., Galluppi, F., Temple, S. & Plana, L. A. Te SpiNNaker Project. *Proc. IEEE* **102**, 652–665 (2014).
- <span id="page-12-52"></span>62. Merolla, P. A. et al. A million spiking-neuron integrated circuit with a scalable communication network and interface. *Science* **345**, 668–673 (2014).
- <span id="page-12-53"></span>63. Davies, M. Putting the 'learning' in machine learning processors: an introduction to the Loihi neuromorphic research chip. *Zenodo* [https://doi.](https://doi.org/10.5281/zenodo.1313406) [org/10.5281/zenodo.1313406](https://doi.org/10.5281/zenodo.1313406) (2018).

- <span id="page-13-33"></span>64. Jo, S. H. et al. Nanoscale memristor device as synapse in neuromorphic systems. *Nano Lett.* **10**, 1297–1301 (2010).
- <span id="page-13-41"></span>65. Ohno, T. et al. Short-term plasticity and long-term potentiation mimicked in single inorganic synapses. *Nat. Mater.* **10**, 591–595 (2011).
- 66. Yu, S., Wu, Y., Jeyasingh, R., Kuzum, D. & Wong, H.-S. P. An electronic synapse device based on metal oxide resistive switching memory for neuromorphic computation. *IEEE Trans. Elect. Dev.* **58**, 2729–2737 (2011).
- 67. Wang, Z. Q. et al. Synaptic learning and memory functions achieved using oxygen ion migration/difusion in an amorphous InGaZnO memristor. *Adv. Funct. Mater.* **22**, 2759–2765 (2012).
- <span id="page-13-34"></span>68. La Barbera, S., Vuillaume, D. & Alibart, F. Filamentary switching: synaptic plasticity through device volatility. *ACS Nano* **9**, 941–949 (2015).
- <span id="page-13-35"></span>69. Stoliar, P. et al. A leaky‐integrate‐and‐fre neuron analog realized with a Mott insulator. *Adv. Funct. Mater.* **27**, 1604740 (2017).
- 70. Al-Shedivat, M., Naous, R., Cauwenberghs, G. & Salama, K. N. Memristors empower spiking neurons with stochasticity. *IEEE Trans. Emerg. Sel. Topics Circuits Syst.* **5**, 242–253 (2015).
- 71. Mehonic, A. & Kenyon, A. J. Emulating the electrical activity of the neuron using a silicon oxide RRAM cell. *Front. Neurosci.* **10**, 57 (2016).
- <span id="page-13-36"></span>72. Zhang, X. et al. An artifcial neuron based on a threshold switching memristor. *IEEE Elect. Dev. Lett.* **39**, 308–311 (2018).
- <span id="page-13-37"></span>73. Pantazi, A., Woźniak, S., Tuma, T. & Elefheriou, E. All-memristive neuromorphic computing with level-tuned neurons. *Nanotechnology* **27**, 355205 (2016).
- <span id="page-13-44"></span>74. Wang, Z. et al. Fully memristive neural networks for pattern classifcation with unsupervised learning. *Nat. Electron.* **1**, 137–145 (2018).
- <span id="page-13-38"></span>75. Pedretti, G. et al. Memristive neural network for on-line learning and tracking with brain-inspired spike timing dependent plasticity. *Sci. Rep.* **7**, 5288 (2017).
- <span id="page-13-39"></span>76. Kim, S. et al. Experimental demonstration of a second-order memristor and its ability to biorealistically implement synaptic plasticity. *Nano Lett.* **15**, 2203–2211 (2015).
- <span id="page-13-40"></span>77. Jerry, M., Parihar, A., Grisafe, B., Raychowdhury, A. & Datta, S. Ultra-low power probabilistic IMT neurons for stochastic sampling machine. In *2017 Symp. VLSI Technology (VLSIT)* T186–T187 (IEEE, 2017).
- <span id="page-13-42"></span>78. Lai, Q. et al. Analog memory capacitor based on feld-confgurable ion-doped polymers. *Appl. Phys. Lett.* **95**, 213503 (2009).
- <span id="page-13-43"></span>79. Nefci, E., Das, S., Pedroni, B., Kreutz-Delgado, K. & Cauwenberghs, G. Event-driven contrastive divergence for spiking neuromorphic systems. *Front. Neurosci.* **7**, 272 (2014).
- <span id="page-13-45"></span>80. Xia, Q. et al. Memristor−CMOS hybrid integrated circuits for reconfgurable logic. *Nano Lett.* **9**, 3640–3645 (2009).
- <span id="page-13-46"></span>81. Pi, S., Lin, P. & Xia, Q. Cross point arrays of 8 nm × 8 nm memristive devices fabricated with nanoimprint lithography. *J. Vacuum Sci. Technol. B* **31**, 06FA02 (2013).
- <span id="page-13-47"></span>82. Shulaker, M. et al. Tree-dimensional integration of nanotechnologies for computing and data storage on a single chip. *Nature* **547**, 74–78 (2017).
- <span id="page-13-48"></span>83. Jang, J. et al. Vertical cell array using TCAT(Terabit Cell Array Transistor) technology for ultra high density NAND fash memory. In *2009 Symp. VLSI Technology (VLSIT)* 192–193 (IEEE, 2009).
- <span id="page-13-49"></span>84. Clarke, P. Report: TSMC to ofer embedded ReRAM in 2019. *eeNews* [http://www.eenewsanalog.com/news/report-tsmc-ofer-embedded](http://www.eenewsanalog.com/news/report-tsmc-offer-embedded-reram-2019)[reram-2019](http://www.eenewsanalog.com/news/report-tsmc-offer-embedded-reram-2019) (2017).
- <span id="page-13-50"></span>85. Yoon, J. H. et al. An artifcial nociceptor based on a difusive memristor. *Nat. Commun.* **9**, 417 (2018).
- <span id="page-13-51"></span>86. Gao, L. et al. Digital-to-analog and analog-to-digital conversion with metal oxide memristors for ultra-low power computing. In *2013 IEEE/ACM Int. Symp. Nanoscale Architectures (NANOARCH)* 19–22 (IEEE, 2013).
- <span id="page-13-0"></span>87. Pi, S., Ghadiri-Sadrabadi, M., Bardin, J. C. & Xia, Q. Nanoscale memristive radiofrequency switches. *Nat. Commun.* **6**, 7519 (2015).
- <span id="page-13-1"></span>88. Li, Z. et al. Experimental demonstration of a defect-tolerant nanocrossbar demultiplexer. *Nanotechnology* **19**, 165203 (2008).
- <span id="page-13-2"></span>89. Bayat, F. M. Implementation of multilayer perceptron network with highly uniform passive memristive crossbar circuits. *Nat. Commun.* **9**, 2331 (2018).
- <span id="page-13-3"></span>90. Du, C. et al. Reservoir computing using dynamic memristors for temporal information processing. *Nat. Commun.* **8**, 2204 (2017).
- <span id="page-13-4"></span>91. Serb, A. et al. Unsupervised learning in probabilistic neural networks with multi-state metal-oxide memristive synapses. *Nat. Commun.* **7**, 12611 (2016).
- <span id="page-13-5"></span>92. Midya, R. et al. Anatomy of Ag/Hafnia-based selectors with 1010 nonlinearity. *Adv. Mater.* **29**, 1604457 (2017).
- <span id="page-13-6"></span>93. Srinivasan, V. S. S. et al. Punchthrough-diode-based bipolar RRAM selector by Si epitaxy. *IEEE Electron Dev. Lett.* **33**, 1396–1398 (2012).
- <span id="page-13-7"></span>94. Huang, J.-J., Tseng, Y.-M., Hsu, C.-W. & Hou, T.-H. Bipolar nonlinear Ni/ TiO2/Ni selector for 1S1R crossbar array applications. *IEEE Electron Dev. Lett.* **32**, 1427–1429 (2011).
- <span id="page-13-8"></span>95. Shin, J. et al. TiO2-based metal–insulator–metal selection device for bipolar resistive random access memory cross-point application. *J. Appl. Phys.* **109**, 033712 (2011).
- <span id="page-13-9"></span>96. Govoreanu, B. et al. High-performance metal–insulator–metal tunnel diode selectors. *IEEE Electron Dev. Lett.* **35**, 63–65 (2014).
- <span id="page-13-10"></span>97. Woo, J. et al. Electrical and reliability characteristics of a scaled (∼30nm) tunnel barrier selector (W/Ta2O5/TaOx/TiO2/TiN) with excellent performance (JMAX>107A/cm2 ). In *2014 Symp. VLSI Technology (VLSIT)* (IEEE, 2014); <https://doi.org/10.1109/VLSIT.2014.6894431>
- <span id="page-13-11"></span>98. Lee, W. et al. Varistor-type bidirectional switch (JMAX>107A/cm2 , selectivity ∼104 ) for 3D bipolar resistive memory arrays. In *2012 Symp. VLSI Technology (VLSIT)* 37–38 (IEEE, 2012).
- <span id="page-13-12"></span>99. Choi, B. J. et al. Trilayer tunnel selectors for memristor memory cells. *Adv. Mater.* **28**, 356 (2016).
- <span id="page-13-13"></span>100. Kawahara, A. et al. An 8 Mb multi-layered cross-point ReRAM macro with 443 MB/s write throughput. *IEEE J. Solid-State Circuits* **48**, 178 (2013).
- <span id="page-13-14"></span>101. *MIEC\* Access Device for 3D-Crosspoint Nonvolatile Memory Arrays* (IBM, 2013); [https://researcher.watson.ibm.com/researcher/fles/us-gwburr/](https://researcher.watson.ibm.com/researcher/files/us-gwburr/MIECOverviewPublicDomain_Jan2013_3.pdf) [MIECOverviewPublicDomain\\_Jan2013\\_3.pdf](https://researcher.watson.ibm.com/researcher/files/us-gwburr/MIECOverviewPublicDomain_Jan2013_3.pdf)
- <span id="page-13-15"></span>102. Govoreanu, B. et al. Termally stable integrated Se-based OTS selectors with >20MA/cm2 current drive, >3.103 half-bias nonlinearity, tunable threshold voltage and excellent endurance. In *2017 Symp. VLSI Technology (VLSIT)* T92–T93 (IEEE, 2017).
- <span id="page-13-16"></span>103. Ohba, K. et al. Cross point Cu-ReRAM with BC-doped selector. In *2018 IEEE Int. Memory Workshop (IMW)* (IEEE, 2018); [https://doi.org/10.1109/](https://doi.org/10.1109/IMW.2018.8388824) [IMW.2018.8388824](https://doi.org/10.1109/IMW.2018.8388824)
- <span id="page-13-17"></span>104. Yasuda, S. et al. A cross point Cu-ReRAM with a novel OTS selector for storage class memory applications. In *2017 Symp. VLSI Technology (VLSIT)* T30–T31 (IEEE, 2017).
- <span id="page-13-18"></span>105. Yang, H. et al. Novel selector for high density non-volatile memory with ultra-low holding voltage and 107 on/of ratio. In *2015 VLSI Technology Symp. (VLSIT)* T130–T131 (IEEE, 2015).
- <span id="page-13-19"></span>106. Kim, S. G. et al. Breakthrough of selector technology for cross-point 25-nm ReRAM. In *2017 Int. Electron Devices Meeting (IEDM)* 2.1.1-2.1.4 (IEEE, 2017).
- <span id="page-13-20"></span>107. Son, M. et al. Excellent selector characteristics of nanoscale VO2 for high-density bipolar ReRAM applications. *IEEE Electron Device Lett.* **32**, 1579–1581 (2011).
- <span id="page-13-21"></span>108. Kim, W. G. et al. NbO2-based low power and cost efective 1S1R switching for high density cross point ReRAM application. In *2014 Symp. VLSI Technology (VLSIT)* (IEEE, 2014); [https://doi.org/10.1109/](https://doi.org/10.1109/VLSIT.2014.6894405) [VLSIT.2014.6894405](https://doi.org/10.1109/VLSIT.2014.6894405)
- <span id="page-13-22"></span>109. Cha, E. et al. Nanoscale (∼10nm) 3D vertical ReRAM and NbO2 threshold selector with TiN electrode. In *2013 Int. Electron Devices Meeting (IEDM)* 10.5.1–10.5.4 (IEEE, 2013).
- <span id="page-13-23"></span>110. Lee, M.-J. et al. Highly-scalable threshold switching select device based on chalcogenide glasses for 3D nanoscaled memory arrays. In *2012 Int. Electron Devices Meeting (IEDM)* 2.6.1–2.6.3. (IEEE, 2012).
- <span id="page-13-24"></span>111. Sun, J. et al. Physically transient threshold switching device based on magnesium oxide for security application. *Small* **14**, 1800945 (2018).
- <span id="page-13-25"></span>112. Jo, S. H., Kumar, T., Narayanan, S., Lu, W. D. & Nazarian, H. 3D-stackable crossbar resistive memory based on feld assisted superlinear threshold (FAST) selector. In 2*014 Int. Electron Devices Meeting (IEDM)* 6.7.1–6.7.4 (IEEE, 2014).
- <span id="page-13-26"></span>113. Ji, L. et al. Integrated one diode–one resistor architecture in nanopillar SiO*x* resistive switching memory by nanosphere lithography. *Nano Lett.* **2**, 14 (2014).
- <span id="page-13-27"></span>114. Wang, G. High‐performance and low‐power rewritable SiO*x* 1 kbit one diode–one resistor crossbar memory array. *Adv. Mater.* **25**, 4789 (2013).
- <span id="page-13-28"></span>115. Govoreanu, B. Vacancy-modulated conductive oxide resistive RAM (VMCO-RRAM): an area-scalable switching current, self-compliant, highly nonlinear and wide on/of-window resistive switching cell. In *2013 Int. Electron Devices Meeting (IEDM)* 10.2.1–10.2.4 (IEEE, 2013).
- <span id="page-13-29"></span>116. Song, M. Self-selective characteristics of nanoscale VO*x* devices for high-density ReRAM applications. *IEEE Electron Dev. Lett.* **33**, 718 (2012).
- <span id="page-13-30"></span>117. Lu, D. et al. Investigations of conduction mechanisms of the self-rectifying n<sup>+</sup>Si-HfO2-Ni RRAM devices. *IEEE Trans. Electron Dev* **61**, 2294–2301 (2014).
- <span id="page-13-31"></span>118. Wang, M. J., Gao, S., Zeng, F., Song, C. & Pan, F. Unipolar resistive switching with forming-free and self-rectifying efects in Cu/HfO2/n-Si devices. *AIP Adv.* **6**, 025007 (2016).
- <span id="page-13-32"></span>119. Kim, K.-H., Jo, S. H., Gaba, S. & Lu, W. Nanoscale resistive memory with intrinsic diode characteristics and long endurance. *Appl. Phys. Lett.* **96**, 053106 (2010).

### **Acknowledgements**

The authors were supported by the US Air Force Research Laboratory, the Air Force Office of Scientific Research, the Defense Advanced Research Projects

Agency, the Intelligence Advanced Research Projects Activity, the National Science Foundation and the Semiconductor Research Consortium. We thank Z. Wang, C. Li, N. Upadhyay, S. Nonnenmann, S. Maji and M. Hardin for help in the preparation of the manuscript.

### **Competing interests**

The authors declare no competing interests.

# **NATuRe MATeRiAls** Review Article

## **Additional information**

**Reprints and permissions information** is available at [www.nature.com/reprints](http://www.nature.com/reprints).

**Correspondence** should be addressed to Q.X. or J.J.Y.

**Publisher's note:** Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

© Springer Nature Limited 2019